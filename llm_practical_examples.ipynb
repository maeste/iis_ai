{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esperimenti pratici con le API di Intelligenza Artificiale\n",
    "\n",
    "Benvenuti alla seconda parte del nostro percorso sull'Intelligenza Artificiale! Dopo aver esplorato la teoria delle reti neurali e dei transformer, oggi metteremo in pratica ciò che abbiamo imparato utilizzando API disponibili gratuitamente.\n",
    "\n",
    "In questo notebook esploreremo:\n",
    "1. Come utilizzare le API di Hugging Face per accedere a modelli pre-addestrati\n",
    "2. Come creare semplici interfacce grafiche con Gradio\n",
    "3. Esempi pratici di applicazioni di AI\n",
    "\n",
    "## Preparazione dell'ambiente\n",
    "\n",
    "Per prima cosa, installiamo le librerie necessarie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installiamo le librerie necessarie\n",
    "!pip install huggingface_hub requests gradio transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utilizzo delle API di Hugging Face\n",
    "\n",
    "Hugging Face offre un \"Inference API\" che permette di utilizzare modelli pre-addestrati senza dover gestire l'infrastruttura. È possibile utilizzare questa API gratuitamente con alcuni limiti di utilizzo.\n",
    "\n",
    "Per prima cosa, vediamo come funziona una semplice richiesta API utilizzando la libreria `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "# URL dell'API di inferenza di Hugging Face\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "\n",
    "# È possibile utilizzare un token personale per aumentare i limiti\n",
    "# Se non lo avete, potete utilizzare l'API senza token con limiti più restrittivi\n",
    "# Per ottenere un token: https://huggingface.co/settings/tokens\n",
    "API_TOKEN = \"\"  # Lasciate vuoto se non avete un token\n",
    "\n",
    "headers = {}\n",
    "if API_TOKEN:\n",
    "    headers[\"Authorization\"] = f\"Bearer {API_TOKEN}\"\n",
    "\n",
    "# Funzione per fare una richiesta al modello\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Esempio di generazione di testo con GPT-2\n",
    "input_text = \"L'intelligenza artificiale è\"\n",
    "output = query({\"inputs\": input_text})\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Output: {output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proviamo con diversi modelli\n",
    "\n",
    "Hugging Face offre una vasta gamma di modelli per diversi compiti. Vediamo alcuni esempi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Funzione più generica per utilizzare diversi modelli\n",
    "def query_model(model_name, inputs, task=\"text-generation\"):\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {}\n",
    "    if API_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {API_TOKEN}\"\n",
    "    \n",
    "    payload = {\"inputs\": inputs}\n",
    "    \n",
    "    # Opzioni aggiuntive per la generazione di testo\n",
    "    if task == \"text-generation\":\n",
    "        payload[\"parameters\"] = {\"max_length\": b0, \"temperature\": 0.7}\n",
    "    \n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classificazione di testo\n",
    "\n",
    "Proviamo un modello di classificazione delle emozioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modello per l'analisi del sentimento\n",
    "sentiment_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Testo da analizzare\n",
    "texts = [\n",
    "    \"I love artificial intelligence!\",\n",
    "    \"This class is boring.\",\n",
    "    \"I'm not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = query_model(sentiment_model, text, task=\"sentiment-analysis\")\n",
    "    print(f\"Testo: {text}\")\n",
    "    print(f\"Risultato: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traduzione automatica\n",
    "\n",
    "Vediamo come utilizzare un modello di traduzione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modello di traduzione\n",
    "translation_model = \"Helsinki-NLP/opus-mt-en-it\"\n",
    "\n",
    "# Testi da tradurre\n",
    "english_texts = [\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Today we will learn how to use AI APIs.\",\n",
    "    \"Programming is fun and creative.\"\n",
    "]\n",
    "\n",
    "for text in english_texts:\n",
    "    result = query_model(translation_model, text, task=\"translation\")\n",
    "    print(f\"Inglese: {text}\")\n",
    "    print(f\"Italiano: {result[0]['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sintesi di testo (Summarization)\n",
    "\n",
    "Vediamo come utilizzare un modello per riassumere testi lunghi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modello di summarization\n",
    "summarization_model = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Testo lungo da riassumere\n",
    "long_text = \"\"\"\n",
    "L'intelligenza artificiale (IA) è un campo dell'informatica che si occupa della creazione di macchine capaci di pensare e agire come esseri umani. Negli ultimi anni, l'IA ha fatto progressi significativi grazie all'aumento della potenza di calcolo, alla disponibilità di grandi quantità di dati e allo sviluppo di algoritmi più sofisticati. Le applicazioni dell'IA spaziano dalla medicina alla finanza, dal trasporto all'intrattenimento.\n",
    "\n",
    "Un sottocampo particolarmente attivo dell'IA è l'apprendimento automatico (machine learning), in cui i sistemi imparano dai dati senza essere esplicitamente programmati. Il deep learning, un sottoinsieme del machine learning basato su reti neurali artificiali con molti strati (deep), ha rivoluzionato settori come la visione artificiale, l'elaborazione del linguaggio naturale e la traduzione automatica.\n",
    "\n",
    "I modelli transformer, introdotti nel 2017 con il paper \"Attention is All You Need\", rappresentano una svolta fondamentale nel campo dell'elaborazione del linguaggio naturale. Questi modelli utilizzano un meccanismo di attenzione per comprendere le relazioni tra le parole in un testo, migliorando significativamente le prestazioni in compiti come la traduzione, la generazione di testo e la risposta a domande.\n",
    "\"\"\"\n",
    "\n",
    "result = query_model(summarization_model, long_text, task=\"summarization\")\n",
    "print(\"Testo originale:\\n\", long_text)\n",
    "print(\"\\nRiassunto:\\n\", result[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Riconoscimento di immagini\n",
    "\n",
    "Vediamo come utilizzare un modello per riconoscere oggetti in un'immagine.\n",
    "Per questo esempio, useremo un'immagine da URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# URL di un'immagine (in questo caso, un'immagine di un cane)\n",
    "image_url = \"https://images.unsplash.com/photo-1517849845537-4d257902454a\"\n",
    "\n",
    "# Scarica l'immagine\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Mostra l'immagine\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Converte l'immagine in bytes\n",
    "with BytesIO() as buffer:\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    image_bytes = buffer.getvalue()\n",
    "\n",
    "# Modello per la classificazione di immagini\n",
    "vision_model = \"google/vit-base-patch16-224\"\n",
    "\n",
    "# URL dell'API per la visione\n",
    "vision_api_url = f\"https://api-inference.huggingface.co/models/{vision_model}\"\n",
    "\n",
    "# Fai la richiesta API\n",
    "headers = {}\n",
    "if API_TOKEN:\n",
    "    headers[\"Authorization\"] = f\"Bearer {API_TOKEN}\"\n",
    "response = requests.post(vision_api_url, headers=headers, data=image_bytes)\n",
    "result = response.json()\n",
    "\n",
    "# Mostra i risultati\n",
    "print(\"L'immagine contiene probabilmente:\")\n",
    "for item in result:\n",
    "    print(f\"{item['label']}: {item['score']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creazione di un'interfaccia con Gradio\n",
    "\n",
    "Ora creiamo un'interfaccia grafica per interagire con i modelli in modo più intuitivo. Utilizzeremo Gradio, che è una libreria Python che permette di creare rapidamente interfacce per modelli di machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Funzione che utilizza il modello di generazione di testo\n",
    "def generate_text(prompt, model_name=\"gpt2\", max_length=100):\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {}\n",
    "    if API_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {API_TOKEN}\"\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_length\": max_length,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    if isinstance(result, list) and len(result) > 0 and 'generated_text' in result[0]:\n",
    "        return result[0]['generated_text']\n",
    "    return str(result)  # In caso di errore, restituisce l'errore come stringa\n",
    "\n",
    "# Creazione dell'interfaccia Gradio per la generazione di testo\n",
    "demo_text = gr.Interface(\n",
    "    fn=generate_text,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=2, placeholder=\"Inserisci un testo iniziale...\"),\n",
    "        gr.Dropdown([\"gpt2\", \"distilgpt2\", \"gpt2-medium\"], label=\"Modello\", value=\"gpt2\"),\n",
    "        gr.Slider(50, 200, value=100, step=10, label=\"Lunghezza massima\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Generatore di Testo con AI\",\n",
    "    description=\"Inserisci un testo iniziale e il modello completerà il resto.\"\n",
    ")\n",
    "\n",
    "# Funzione che utilizza il modello di traduzione\n",
    "def translate_text(text, source_lang, target_lang):\n",
    "    # Determina il modello in base alle lingue selezionate\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\"\n",
    "    \n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {}\n",
    "    if API_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {API_TOKEN}\"\n",
    "    \n",
    "    payload = {\"inputs\": text}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        result = response.json()\n",
    "        \n",
    "        if isinstance(result, list) and len(result) > 0 and 'translation_text' in result[0]:\n",
    "            return result[0]['translation_text']\n",
    "        return str(result)  # In caso di errore, restituisce l'errore come stringa\n",
    "    except Exception as e:\n",
    "        return f\"Errore: {str(e)}. Verifica che il modello per la coppia di lingue {source_lang}-{target_lang} esista.\"\n",
    "\n",
    "# Creazione dell'interfaccia Gradio per la traduzione\n",
    "demo_translation = gr.Interface(\n",
    "    fn=translate_text,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=4, placeholder=\"Inserisci un testo da tradurre...\"),\n",
    "        gr.Dropdown([\"en\", \"it\", \"fr\", \"es\", \"de\"], label=\"Lingua di origine\", value=\"en\"),\n",
    "        gr.Dropdown([\"it\", \"en\", \"fr\", \"es\", \"de\"], label=\"Lingua di destinazione\", value=\"it\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Traduttore con AI\",\n",
    "    description=\"Inserisci un testo e seleziona le lingue per tradurlo.\"\n",
    ")\n",
    "\n",
    "# Funzione per l'analisi del sentimento\n",
    "def analyze_sentiment(text):\n",
    "    model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    \n",
    "    headers = {}\n",
    "    if API_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {API_TOKEN}\"\n",
    "    \n",
    "    payload = {\"inputs\": text}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        result = response.json()\n",
    "        \n",
    "        if isinstance(result, list) and len(result) > 0 and 'label' in result[0]:\n",
    "            sentiment = result[0]['label']\n",
    "            score = result[0]['score']\n",
    "            return f\"Sentimento: {sentiment}\\nConfidenza: {score*100:.2f}%\"\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Errore: {str(e)}\"\n",
    "\n",
    "# Creazione dell'interfaccia Gradio per l'analisi del sentimento\n",
    "demo_sentiment = gr.Interface(\n",
    "    fn=analyze_sentiment,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Inserisci un testo in inglese per analizzare il sentimento...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Analisi del Sentimento con AI\",\n",
    "    description=\"Inserisci un testo in inglese per determinare se esprime un sentimento positivo o negativo.\"\n",
    ")\n",
    "\n",
    "# Unisci tutte le demo in un'unica interfaccia con schede\n",
    "demo = gr.TabbedInterface(\n",
    "    [demo_text, demo_translation, demo_sentiment],\n",
    "    [\"Generazione di Testo\", \"Traduzione\", \"Analisi del Sentimento\"],\n",
    "    title=\"Esperimenti con le API di AI\"\n",
    ")\n",
    "\n",
    "# Avvia l'interfaccia\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus: Utilizzo della libreria Transformers di Hugging Face\n",
    "\n",
    "Finora abbiamo utilizzato le API remote di Hugging Face. Se avete abbastanza potenza di calcolo (idealmente una GPU), potete anche eseguire i modelli localmente utilizzando la libreria `transformers`.\n",
    "\n",
    "Ecco un esempio di come utilizzare un modello di generazione di testo localmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Crea un pipeline di generazione di testo\n",
    "# Nota: quando eseguite questo codice per la prima volta, il modello verrà scaricato\n",
    "# (può richiedere un po' di tempo e spazio su disco)\n",
    "text_generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "# Genera testo\n",
    "result = text_generator(\"L'intelligenza artificiale è\", max_length=50, do_sample=True, temperature=0.7)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizi per gli studenti\n",
    "\n",
    "1. **Esercizio base**: Modifica il prompt nel generatore di testo e osserva come cambia l'output.\n",
    "2. **Esercizio intermedio**: Crea un'interfaccia Gradio per il modello di riassunto di testo.\n",
    "3. **Esercizio avanzato**: Integra un modello di generazione di immagini come Stable Diffusion utilizzando le API di Hugging Face.\n",
    "\n",
    "## Conclusioni\n",
    "\n",
    "In questo notebook, abbiamo esplorato come utilizzare le API di Hugging Face per accedere a modelli di intelligenza artificiale e come creare semplici interfacce grafiche con Gradio. Questi strumenti rendono l'IA accessibile anche senza una grande potenza di calcolo o conoscenze approfondite.\n",
    "\n",
    "Ricordate che l'utilizzo delle API di Hugging Face ha dei limiti nella versione gratuita, ma è sufficiente per scopi educativi e sperimentali.\n",
    "\n",
    "## Risorse aggiuntive\n",
    "\n",
    "- [Hugging Face - Inference API](https://huggingface.co/inference-api)\n",
    "- [Documentazione di Gradio](https://www.gradio.app/docs/)\n",
    "- [Hugging Face - Transformers](https://huggingface.co/docs/transformers/index)\n",
    "- [Hugging Face Spaces](https://huggingface.co/spaces) - Esempi di applicazioni create con Gradio e Streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
