{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Come funzionano i Large Language Models (LLM)\n",
    "\n",
    "Questo notebook esplora il funzionamento interno dei Large Language Models, con particolare attenzione all'architettura Transformer che sta alla base di modelli come GPT, BERT, LLaMA, Claude e altri modelli moderni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dai neuroni biologici ai neuroni artificiali\n",
    "\n",
    "### Il neurone biologico\n",
    "\n",
    "![Neurone biologico](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/1200px-Neuron.svg.png)\n",
    "\n",
    "I neuroni biologici sono le cellule fondamentali del sistema nervoso. Funzionano ricevendo segnali (input) attraverso i dendriti, elaborando questi segnali nel corpo cellulare, e trasmettendo l'output attraverso l'assone ad altri neuroni.\n",
    "\n",
    "**Componenti principali:**\n",
    "- **Dendriti**: Ricevono segnali da altri neuroni\n",
    "- **Corpo cellulare**: Elabora i segnali ricevuti\n",
    "- **Assone**: Trasmette il segnale elaborato ad altri neuroni\n",
    "- **Sinapsi**: Connessioni tra neuroni dove avviene lo scambio di informazioni\n",
    "\n",
    "### Il neurone artificiale\n",
    "\n",
    "![Neurone artificiale](https://miro.medium.com/max/1400/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n",
    "\n",
    "Il neurone artificiale è una semplificazione matematica del neurone biologico, progettato per imitare il suo comportamento fondamentale.\n",
    "\n",
    "**Componenti principali:**\n",
    "- **Input (x₁, x₂, ..., xₙ)**: Dati in ingresso, simili ai segnali ricevuti dai dendriti\n",
    "- **Pesi (w₁, w₂, ..., wₙ)**: Parametri che determinano l'importanza di ciascun input\n",
    "- **Funzione di somma**: Aggrega gli input ponderati (∑ wᵢxᵢ + b)\n",
    "- **Bias (b)**: Un termine che permette di regolare la soglia di attivazione\n",
    "- **Funzione di attivazione**: Trasforma la somma ponderata in un output (simile all'assone)\n",
    "- **Output (y)**: Il risultato finale dell'elaborazione\n",
    "\n",
    "La formula matematica di base è: y = f(∑ wᵢxᵢ + b), dove f è la funzione di attivazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reti Neurali\n",
    "\n",
    "### Struttura di una rete neurale\n",
    "\n",
    "![Rete neurale](https://miro.medium.com/max/2000/1*3fA77_mLNiJTSgZFhYnU0Q.png)\n",
    "\n",
    "Una rete neurale è composta da più neuroni artificiali organizzati in strati (layer):\n",
    "\n",
    "1. **Input Layer**: Riceve i dati iniziali\n",
    "2. **Hidden Layers**: Elaborano i dati attraverso trasformazioni successive\n",
    "3. **Output Layer**: Produce il risultato finale\n",
    "\n",
    "Ogni neurone in uno strato è collegato a tutti i neuroni dello strato successivo (in una rete fully connected).\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Il processo di forward propagation è come le informazioni fluiscono dall'input all'output:\n",
    "\n",
    "1. I dati entrano nell'input layer\n",
    "2. Ogni neurone calcola il suo output usando la formula y = f(∑ wᵢxᵢ + b)\n",
    "3. Questi output diventano input per i neuroni dello strato successivo\n",
    "4. Il processo continua fino all'output layer\n",
    "\n",
    "### Backpropagation e addestramento\n",
    "\n",
    "![Backpropagation](https://miro.medium.com/max/1400/1*3fA77_mLNiJTSgZFhYnU0Q.png)\n",
    "\n",
    "L'addestramento di una rete neurale avviene attraverso un processo chiamato backpropagation:\n",
    "\n",
    "1. Si calcola l'errore tra l'output previsto e quello desiderato\n",
    "2. L'errore viene propagato all'indietro attraverso la rete\n",
    "3. I pesi vengono aggiornati per minimizzare l'errore\n",
    "4. Il processo viene ripetuto molte volte con diversi esempi di dati\n",
    "\n",
    "L'algoritmo di ottimizzazione più comune è la discesa del gradiente (gradient descent), che aggiorna i pesi nella direzione che riduce l'errore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Architettura Transformer\n",
    "\n",
    "### Introduzione ai Transformer\n",
    "\n",
    "![Transformer Architecture](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)\n",
    "\n",
    "L'architettura Transformer, introdotta nel paper \"Attention is All You Need\" (2017), ha rivoluzionato l'elaborazione del linguaggio naturale. A differenza dei modelli ricorrenti (RNN, LSTM), i Transformer elaborano l'intera sequenza contemporaneamente, non parola per parola.\n",
    "\n",
    "I principali vantaggi sono:\n",
    "- Parallelizzazione (addestramento più veloce)\n",
    "- Gestione migliore delle dipendenze a lungo termine\n",
    "- Prestazioni superiori su molti compiti linguistici\n",
    "\n",
    "I Transformer sono composti da due componenti principali:\n",
    "1. **Encoder**: Elabora l'input e ne crea una rappresentazione\n",
    "2. **Decoder**: Genera l'output basandosi sulla rappresentazione creata dall'encoder\n",
    "\n",
    "Modelli come BERT usano principalmente l'encoder, mentre modelli come GPT usano principalmente il decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "![Word Embeddings](https://jalammar.github.io/images/t/transformer_embedding_vector.png)\n",
    "\n",
    "Prima di entrare nel Transformer, le parole vengono convertite in vettori numerici chiamati \"embeddings\".\n",
    "\n",
    "**Perché gli embeddings?**\n",
    "- Le reti neurali lavorano con numeri, non con testo\n",
    "- Gli embeddings catturano relazioni semantiche tra le parole\n",
    "- Parole simili hanno embeddings simili nello spazio vettoriale\n",
    "\n",
    "Ogni parola viene rappresentata da un vettore di dimensione fissa (es. 512 numeri in float). Questi vettori non sono assegnati casualmente, ma vengono appresi durante l'addestramento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "![Positional Encoding](https://jalammar.github.io/images/t/transformer_positional_encoding_example.png)\n",
    "\n",
    "A differenza dei modelli ricorrenti, i Transformer non hanno un concetto intrinseco dell'ordine delle parole. Per questo motivo, viene aggiunto un \"positional encoding\" agli embeddings.\n",
    "\n",
    "**Funzionamento:**\n",
    "- A ogni posizione nella sequenza è associato un vettore unico\n",
    "- Questi vettori sono generati usando funzioni sinusoidali\n",
    "- Il positional encoding viene sommato agli embeddings delle parole\n",
    "- Questo permette al modello di capire la posizione relativa delle parole nella frase\n",
    "\n",
    "Formula: \n",
    "- PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "- PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "\n",
    "Dove pos è la posizione e i è la dimensione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "![Multi-Head Attention](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "\n",
    "Il cuore del Transformer è il meccanismo di attenzione (attention), che permette al modello di focalizzarsi su parti diverse dell'input quando genera ogni parola dell'output.\n",
    "\n",
    "**Componenti del meccanismo di attenzione:**\n",
    "- **Query (Q)**: Ciò che stiamo cercando\n",
    "- **Key (K)**: Ciò con cui confrontiamo la query\n",
    "- **Value (V)**: Il valore effettivo che viene estratto\n",
    "\n",
    "**Processo:**\n",
    "1. Per ogni posizione, calcoliamo un punteggio di attenzione tra la sua query e tutte le keys\n",
    "2. Applichiamo softmax per ottenere pesi di attenzione\n",
    "3. Moltiplichiamo i pesi per i values e sommiamo\n",
    "4. Il risultato è un vettore che incorpora informazioni da tutta la sequenza, con enfasi sulle parti più rilevanti\n",
    "\n",
    "**Multi-Head:**\n",
    "- L'attenzione viene calcolata più volte in parallelo (\"heads\")\n",
    "- Ogni head può focalizzarsi su relazioni diverse nell'input\n",
    "- I risultati dei diversi heads vengono combinati\n",
    "\n",
    "Formula dell'attenzione: Attention(Q, K, V) = softmax(QK^T/√d_k)V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks\n",
    "\n",
    "![Feed-Forward Network](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n",
    "\n",
    "Dopo il meccanismo di attenzione, ogni posizione passa attraverso una rete feed-forward composta da due trasformazioni lineari con una funzione ReLU nel mezzo.\n",
    "\n",
    "**Caratteristiche:**\n",
    "- Viene applicata indipendentemente a ogni posizione\n",
    "- Introduce non-linearità nel modello\n",
    "- Aumenta la capacità rappresentativa del modello\n",
    "\n",
    "Formula: FFN(x) = max(0, xW₁ + b₁)W₂ + b₂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization e Residual Connections\n",
    "\n",
    "![Layer Norm and Residual](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "\n",
    "Per migliorare l'addestramento e le prestazioni, i Transformer utilizzano due tecniche importanti:\n",
    "\n",
    "**Layer Normalization:**\n",
    "- Normalizza gli input di ogni layer per avere media 0 e varianza 1\n",
    "- Stabilizza l'addestramento\n",
    "- Riduce la dipendenza dalla scala degli input\n",
    "\n",
    "**Residual Connections (Skip Connections):**\n",
    "- Aggiunge l'input originale all'output di un sotto-layer\n",
    "- Aiuta a risolvere il problema del vanishing gradient\n",
    "- Permette l'addestramento di reti più profonde\n",
    "\n",
    "Formula: LayerNorm(x + Sublayer(x))\n",
    "\n",
    "Queste tecniche vengono applicate sia al blocco di attenzione che alla rete feed-forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Architettura completa del Transformer\n",
    "\n",
    "![Complete Transformer](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)\n",
    "\n",
    "Il Transformer completo è composto da:\n",
    "\n",
    "**Encoder:**\n",
    "- N blocchi identici (tipicamente 6 nel paper originale)\n",
    "- Ogni blocco contiene:\n",
    "  1. Multi-Head Self-Attention\n",
    "  2. Feed-Forward Network\n",
    "  3. Layer Normalization e Residual Connections\n",
    "\n",
    "**Decoder:**\n",
    "- N blocchi identici\n",
    "- Ogni blocco contiene:\n",
    "  1. Masked Multi-Head Self-Attention (per impedire che il modello \"veda il futuro\")\n",
    "  2. Multi-Head Attention che collega decoder ed encoder\n",
    "  3. Feed-Forward Network\n",
    "  4. Layer Normalization e Residual Connections\n",
    "\n",
    "**Strato finale:**\n",
    "- Linear Layer\n",
    "- Softmax (per convertire i punteggi in probabilità)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Addestramento e utilizzo di un LLM\n",
    "\n",
    "### Pre-training\n",
    "\n",
    "![Pre-training](https://jalammar.github.io/images/gpt3/01-gpt3-training-setup.png)\n",
    "\n",
    "I Large Language Models vengono inizialmente addestrati su enormi quantità di testo attraverso un processo chiamato \"pre-training\".\n",
    "\n",
    "**Caratteristiche del pre-training:**\n",
    "- **Obiettivo di apprendimento**: Spesso è la previsione della parola successiva (causal language modeling)\n",
    "- **Dataset**: Centinaia di gigabyte o terabyte di testo da internet, libri, articoli, codice, ecc.\n",
    "- **Durata**: Può richiedere settimane o mesi su cluster di GPU/TPU\n",
    "- **Self-supervised**: Non richiede etichette manuali, il testo stesso fornisce la \"supervisione\"\n",
    "\n",
    "Durante questa fase, il modello impara:\n",
    "- Strutture sintattiche e grammaticali\n",
    "- Relazioni semantiche tra concetti\n",
    "- Conoscenze fattuali del mondo\n",
    "- Ragionamento di base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "![Fine-tuning](https://miro.medium.com/max/1400/1*YEJf9BQQh0ma1JjdwkdIIw.png)\n",
    "\n",
    "Dopo il pre-training, i modelli possono essere specializzati per compiti specifici attraverso il \"fine-tuning\".\n",
    "\n",
    "**Caratteristiche del fine-tuning:**\n",
    "- **Dataset**: Più piccolo e task-specific (es. domande e risposte, riassunti, traduzioni)\n",
    "- **Durata**: Molto più breve del pre-training (ore o giorni)\n",
    "- **Supervised**: Richiede dati etichettati per il compito specifico\n",
    "\n",
    "**Tipi di fine-tuning:**\n",
    "- **Standard Fine-tuning**: Aggiornamento di tutti i parametri\n",
    "- **Parameter-Efficient Fine-tuning (PEFT)**: Aggiornamento solo di un sottoinsieme di parametri (es. LoRA, Adapters)\n",
    "- **Instruction Tuning**: Addestramento su input formattati come istruzioni per migliorare la capacità di seguire direttive\n",
    "- **RLHF (Reinforcement Learning from Human Feedback)**: Utilizzo di feedback umano per allineare il modello a preferenze umane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferenza\n",
    "\n",
    "![Inference](https://jalammar.github.io/images/gpt3/15-input-embedding-positioning.png)\n",
    "\n",
    "L'inferenza è il processo di utilizzo del modello addestrato per generare testo.\n",
    "\n",
    "**Processo di generazione del testo:**\n",
    "1. Il prompt dell'utente viene tokenizzato e convertito in embeddings\n",
    "2. Il modello elabora questi embeddings\n",
    "3. Calcola una distribuzione di probabilità sulla parola successiva\n",
    "4. Seleziona la parola successiva (usando sampling, beam search, ecc.)\n",
    "5. Aggiunge la parola selezionata al contesto\n",
    "6. Ripete i passaggi 2-5 fino a raggiungere una condizione di stop\n",
    "\n",
    "**Tecniche di decodifica:**\n",
    "- **Greedy Decoding**: Scegli sempre la parola con probabilità più alta\n",
    "- **Beam Search**: Mantieni le k sequenze più probabili\n",
    "- **Sampling**: Scegli parole in base alla loro probabilità\n",
    "- **Top-k Sampling**: Limita il sampling alle k parole più probabili\n",
    "- **Top-p (Nucleus) Sampling**: Limita il sampling alle parole che costituiscono una probabilità cumulativa p\n",
    "- **Temperature**: Controlla la casualità dell'output (temperature più alte = più casualità)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evoluzione dei LLM\n",
    "\n",
    "![LLM Evolution](https://miro.medium.com/max/1400/1*HxWv8UK9-E6Dq-RCTvGcHg.png)\n",
    "\n",
    "I Large Language Models hanno avuto un'evoluzione rapida negli ultimi anni:\n",
    "\n",
    "**GPT (2018)**: 117M parametri\n",
    "- Prima generazione di modelli basati su Transformer decoder-only\n",
    "- Approccio generativo autoregressive\n",
    "\n",
    "**BERT (2018)**: 340M parametri\n",
    "- Bidirectional Encoder Representations from Transformers\n",
    "- Encoder-only, ottimizzato per la comprensione del testo\n",
    "\n",
    "**GPT-2 (2019)**: 1.5B parametri\n",
    "- Scaling up di GPT\n",
    "- Migliore generazione di testo coerente\n",
    "\n",
    "**GPT-3 (2020)**: 175B parametri\n",
    "- Enorme aumento di scala\n",
    "- Emergenza di capacità in-context learning (few-shot, one-shot, zero-shot)\n",
    "\n",
    "**InstructGPT & ChatGPT (2022)**: Basati su GPT-3.5\n",
    "- Fine-tuning con RLHF per seguire istruzioni e allinearsi a preferenze umane\n",
    "- Ottimizzati per il dialogo\n",
    "\n",
    "**GPT-4, Claude, LLaMA, Gemini (2023-2024)**: Fino a trilioni di parametri\n",
    "- Miglioramenti in ragionamento, conoscenza, multitasking\n",
    "- Capacità multimodali (testo, immagini, audio)\n",
    "- Maggiore allineamento con valori umani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limitazioni e sfide\n",
    "\n",
    "![Challenges](https://miro.medium.com/max/1400/1*HfPmhfjVr5ZiJ1Y0QPeqhQ.png)\n",
    "\n",
    "Nonostante i progressi, i LLM presentano ancora diverse limitazioni:\n",
    "\n",
    "**Allucinazioni**:\n",
    "- Generazione di informazioni false ma plausibili\n",
    "- Difficoltà nel distinguere fatti da speculazioni\n",
    "\n",
    "**Bias e tossicità**:\n",
    "- Riflettono i bias presenti nei dati di addestramento\n",
    "- Possono generare contenuti discriminatori o inappropriati\n",
    "\n",
    "**Ragionamento complesso**:\n",
    "- Difficoltà con calcoli matematici complessi\n",
    "- Limiti nel ragionamento logico a più passaggi\n",
    "\n",
    "**Contesto limitato**:\n",
    "- Finestra di contesto finita (es. 2048, 4096, 8192, 32k token)\n",
    "- Difficoltà nel mantenere coerenza su testi molto lunghi\n",
    "\n",
    "**Comprensione del mondo reale**:\n",
    "- Mancanza di esperienza diretta con il mondo fisico\n",
    "- Comprensione superficiale di concetti che richiedono esperienza embodied\n",
    "\n",
    "**Aggiornamento delle conoscenze**:\n",
    "- Conoscenze limitate al periodo di addestramento\n",
    "- Difficoltà nell'aggiornare le conoscenze senza riaddestramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Applicazioni dei LLM\n",
    "\n",
    "![Applications](https://miro.medium.com/max/1400/1*HxWv8UK9-E6Dq-RCTvGcHg.png)\n",
    "\n",
    "I Large Language Models hanno trovato applicazione in numerosi campi:\n",
    "\n",
    "**Assistenti virtuali**:\n",
    "- Chatbot conversazionali (ChatGPT, Claude, Bard)\n",
    "- Assistenti personali e aziendali\n",
    "\n",
    "**Creazione di contenuti**:\n",
    "- Copywriting e marketing\n",
    "- Scrittura creativa (storie, poesie, sceneggiature)\n",
    "- Generazione di codice e documentazione tecnica\n",
    "\n",
    "**Educazione**:\n",
    "- Tutoring personalizzato\n",
    "- Creazione di materiale didattico\n",
    "- Valutazione di elaborati\n",
    "\n",
    "**Ricerca e analisi**:\n",
    "- Sintesi di letteratura scientifica\n",
    "- Analisi di documenti legali\n",
    "- Estrazione di informazioni da grandi corpus di testo\n",
    "\n",
    "**Traduzione e localizzazione**:\n",
    "- Traduzione automatica di alta qualità\n",
    "- Adattamento culturale dei contenuti\n",
    "\n",
    "**Accessibilità**:\n",
    "- Trascrizione e sottotitolazione\n",
    "- Assistenza per persone con disabilità\n",
    "\n",
    "**Sviluppo software**:\n",
    "- Generazione e debugging di codice\n",
    "- Assistenza alla programmazione\n",
    "- Conversione tra linguaggi di programmazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Futuro dei LLM\n",
    "\n",
    "![Future](https://miro.medium.com/max/1400/1*3fA77_mLNiJTSgZFhYnU0Q.png)\n",
    "\n",
    "Il campo dei Large Language Models continua a evolversi rapidamente. Alcune direzioni future includono:\n",
    "\n",
    "**Modelli multimodali**:\n",
    "- Integrazione di testo, immagini, audio e video\n",
    "- Comprensione e generazione su più modalità\n",
    "\n",
    "**Ragionamento avanzato**:\n",
    "- Miglioramento delle capacità di ragionamento logico e matematico\n",
    "- Integrazioni con strumenti esterni (calcolatrici, basi di conoscenza)\n",
    "\n",
    "**Efficienza computazionale**:\n",
    "- Modelli più piccoli ma ugualmente capaci\n",
    "- Tecniche di distillazione e quantizzazione\n",
    "- Architetture più efficienti (Mixture of Experts, Sparse Attention)\n",
    "\n",
    "**Affidabilità e sicurezza**:\n",
    "- Riduzione delle allucinazioni\n",
    "- Miglioramento della sicurezza contro prompt malevoli\n",
    "- Maggiore trasparenza e interpretabilità\n",
    "\n",
    "**Personalizzazione e adattamento**:\n",
    "- Modelli adattabili a utenti e domini specifici\n",
    "- Fine-tuning più efficiente e accessibile\n",
    "\n",
    "**Integrazione con il mondo reale**:\n",
    "- Connessione con robot e sistemi fisici\n",
    "- Interazione con ambienti digitali in tempo reale\n",
    "- Capacità di pianificazione e esecuzione di azioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusioni\n",
    "\n",
    "I Large Language Models rappresentano una delle più significative rivoluzioni nell'ambito dell'intelligenza artificiale degli ultimi anni. Costruiti sulla base dell'architettura Transformer, questi modelli hanno dimostrato capacità sorprendenti nel comprendere e generare linguaggio naturale.\n",
    "\n",
    "**Punti chiave:**\n",
    "- L'architettura Transformer ha superato i precedenti approcci nel NLP\n",
    "- Il meccanismo di attenzione permette ai modelli di catturare relazioni complesse nel testo\n",
    "- La scala (numero di parametri e dati di addestramento) ha portato all'emergere di nuove capacità\n",
    "- Il fine-tuning e l'allineamento hanno reso i modelli più utili e sicuri\n",
    "- Nonostante i progressi, esistono ancora importanti limitazioni da superare\n",
    "\n",
    "Con lo sviluppo continuo della ricerca e della tecnologia, i Large Language Models continueranno ad evolvere, aprendo nuove possibilità per l'interazione uomo-macchina e la risoluzione di problemi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
